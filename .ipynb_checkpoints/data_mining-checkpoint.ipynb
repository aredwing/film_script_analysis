{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Film Script Analyzer, by Luis Castro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The aim of this project is to create a web application capable of processing a film script, that is, analyze its syntactic and semantic characteristics along with additional meta-data. \n",
    "\n",
    "This is done by:\n",
    "- Scrapping the web for scripts to build and keep growing the available data.\n",
    "- Request meta-data from IMDB API to receive general information from a film, that information is related to a film script. Along with this information is the IMDB rating, our target value to predict. \n",
    "- Information from the text of the script is sent to Watson's API to its Personality Insights application which returns a vector of 30 float numbers that describes features of the text.\n",
    "- Additionaly, using the NLTK library, a NLP tool further information is created to enrich the dataset.\n",
    "\n",
    "When the dataset is ready:\n",
    "- Data is preprocessed, that is, checked for missing values, scaled as it is needed for many machine learning algorithms that calculate distances among instances.\n",
    "- Most relevant features are selected by feature selection and variance explained.\n",
    "- Data is split into training and test sets\n",
    "- A set of supervised learning algorithms are tested individualy and as an ensamble to get the best prediction posible.\n",
    "- Accuracy of the model is tested using Root Mean Square Error.\n",
    "\n",
    "By developing and refining the application, it can be a valuable tool to quickly evaluate a film script, providing valuable insight to aspiring and profesional writers alike, it will also prove to be a valuable ally to producers or directors that need to read many of those, by setting a guideline or baseline to guide their efforts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Libraries to be used.\n",
    "# The helper library was developed for this project.\n",
    "import helper as hp\n",
    "import pandas as pd\n",
    "import numpy  as np\n",
    "import bs4\n",
    "import string\n",
    "from sklearn.linear_model import LinearRegression as lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Done for testing purposes, a subset of film scripts was selected randomly.\n",
    "np.random.seed(0)\n",
    "n = 1\n",
    "letters   = list(np.random.choice(list(string.ascii_uppercase),size=n,replace=False))\n",
    "url       = [\"http://www.springfieldspringfield.co.uk/movie_scripts.php?order=\",\"&page=\"]\n",
    "\n",
    "# Append data obtained to empty list\n",
    "# fetch the url, and extract the links from it\n",
    "# these links contain the droids we are looking for... i mean the scripts.\n",
    "prefix = 'http://www.springfieldspringfield.co.uk'\n",
    "pages  = {}\n",
    "n      = 3\n",
    "for i in letters:\n",
    "    for j in range(1,n): # Number of pages to go in, change latter too.\n",
    "        temp = bs4.BeautifulSoup(hp.fetch(url[0]+i+url[1]+str(j)).text,'lxml')\n",
    "        temp = temp.find_all('a',class_='script-list-item')\n",
    "        clean = ''\n",
    "        for link in temp:\n",
    "            pages.update({str(link.contents[0]):prefix+link.get('href')})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Having the list of url's, got to each one and scrape the script\n",
    "# the sprintScrap was specifically designed to scrape information\n",
    "# form that webpage.\n",
    "script = []\n",
    "for i in pages.values():\n",
    "    script.append(hp.springScrap(hp.fetch(i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Process script to remove all caps words, indicating actions\n",
    "# to be seen if leaving or removing them creates a better model.\n",
    "scriptRC = []\n",
    "for i in range(len(script)):\n",
    "    scriptRC.append(hp.removeCAPS(script[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save text of scrapped and process scripts to local disk.\n",
    "for i in range(len(script)):\n",
    "    f = open('scrapped/'+pages.keys()[i]+'.txt','w')\n",
    "    f.write(script[i])\n",
    "    f.close()\n",
    "    \n",
    "for i in range(len(script)):\n",
    "    f = open('scrapped/'+pages.keys()[i]+'RC.txt','w')\n",
    "    f.write(scriptRC[i])\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Username and password for Watson's API, obtained\n",
    "# Creating an account with them, some free perks.\n",
    "iusername = 'ABCDEF'\n",
    "ipassword = '123456'\n",
    "\n",
    "# Submit scripts (in this case those with actions removed)\n",
    "# to Watson through the API, the API is for Personality Insights\n",
    "# it returns a set of 30 parameters obtained by analysing\n",
    "# the text supplied.\n",
    "insights = []\n",
    "for i in scriptRC:\n",
    "    insights.append(hp.insight(i,iusername,ipassword))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Associate names of movie scripts with insights returned.\n",
    "nins = []\n",
    "for i in range(len(insights)):\n",
    "    nins.append([pages.keys()[i],hp.dToL(hp.flatten(insights[i]))[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a dataframe, the columns will be the each of the insights features\n",
    "# the rows each instant of movie script, it starts with 0's.\n",
    "index   = pages.keys()\n",
    "columns = hp.dToL(hp.flatten(insights[0]))[1]\n",
    "\n",
    "df = pd.DataFrame(data=np.zeros(shape=(len(index),len(columns))),index=index,columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Fill the dataframe with the information from the insights.\n",
    "for i in range(len(index)):\n",
    "    for j in range(len(columns)):\n",
    "        df.ix[(i,j)] = nins[i][1][j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# After completing receiving data from Watson, proceed to use OMDB API, this API\n",
    "# returns important data from a movie by supplying the name of the movie or\n",
    "# the imdb tag. The names of the movies were available from the previous scrapping, \n",
    "# however names sometimes are spelled or written differently and could not be found \n",
    "# by the API, that is why this was checked by hand.\n",
    "\n",
    "# Some curation of the titles needs to be done by hand, will automate (somehow) later\n",
    "titles = ['Ca$h','Caddyshack','Cabin Fever 3: Patient Zero','Caged','Cairo Time',\n",
    "          'Caddyshack II','Cable Guy','Caffeine','Caligula','Calendar Girls',\n",
    "          'Cabin in the sky','The White Horse','Cadaver','Cabin Fever','Call Girl',\n",
    "          'C.r.a.z.y','Julius Caesar','Caesar and Cleopatra','Cake','Cabin Fever 2',\n",
    "          'California Solo','Cabaret Desire','C.O.G.','Cafe Society','Cadillac Records',\n",
    "          'Cabin in the woods','Cabaret','Illustrious Corpses','Cadillac Man','Cahill',\n",
    "          'Cake Eaters','Calamity Jane','The Dark Knight','The Godfather','Fight Club',\n",
    "          'The Lord of the Rings: The Fellowship of the Ring',\n",
    "          'Star Wars: Episode V - The Empire Strikes Back','The Matrix',\n",
    "          'The silence of the lambs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set the dataframe index to the curated names, the pandas dataframe\n",
    "# will have the film names as index and the features as columns.\n",
    "df.index = titles\n",
    "\n",
    "# Proceed to request the information to the OMDB API and store it in a dictionary.\n",
    "imdbDict= {}\n",
    "for i in titles:\n",
    "    imdbDict.update({i:omdb(name=i)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Our target is in sight, the 'imdbRating' is what the app will\n",
    "# predict through regression. Create the target and store it in memory.\n",
    "target = pd.DataFrame(index=df.index,columns=['imdbRating'])\n",
    "for i in df.index:\n",
    "    target.ix[i] = imdbDict[i]['imdbRating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Check if the rating was available, if not remove that row, as\n",
    "# there will be no way of evaluating that instance.\n",
    "nas    = target['imdbRating']!='N/A'\n",
    "target = target.ix[nas]\n",
    "df     = df.ix[nas]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# By using the NLTK library for Natural Language Processing\n",
    "# it is possible to obtain interesting statistics of the text\n",
    "# that can give further power to the model analytics \n",
    "# the model now uses count of words, diversity of words = (unique words/total words) \n",
    "# and verbs/nouns.\n",
    "nanalytics     = []\n",
    "for i in range(len(scriptRC)):\n",
    "    nanalytics.append([pages.keys()[i],words(scriptRC[i])])\n",
    "\n",
    "# Store this new analytics in the training df.\n",
    "df['Words']      = 0.\n",
    "df['DiversityW'] = 0.\n",
    "df['Verb/Noun']  = 0.\n",
    "\n",
    "for i in range(len(nanalytics)):\n",
    "    df['Words'].iloc[i]      = nanalytics[i][1][0]\n",
    "    df['DiversityW'].iloc[i] = nanalytics[i][1][1]\n",
    "    df['Verb/Noun'].iloc[i]  = nanalytics[i][1][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# From the IMDB API information we use 2 features so far,\n",
    "# year and the runtime of the movie.\n",
    "df['Year']    = 0\n",
    "df['Runtime'] = 0\n",
    "\n",
    "for i in df.index:\n",
    "    df['Year'][i]    = imdbDict[i]['Year']\n",
    "    df['Runtime'][i] = imdbDict[i]['Runtime'].split()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Finish by storing the dataset and the target on disk for further use.\n",
    "df.to_csv('scrapped/final.csv')\n",
    "target.to_csv('scrapped/target.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
