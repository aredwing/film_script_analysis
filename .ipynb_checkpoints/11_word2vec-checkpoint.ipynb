{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Film Script Analyzer\n",
    "\n",
    "##Â Gensim (word2vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word vectors have gained much attention by their capacity to translate words, in an unsupervised way depending on their context in vectors. Vectors then can be used to find similarities between words, words close by in the vector space are similar words. Perform addition or subtraction of words to create new words.\n",
    "\n",
    "A classic example is King - man + woman = Queen.\n",
    "\n",
    "Another way may be, in the context of film scripts, to find similar characters in different movies. Find threads in stories as numeric sucessions, etc.\n",
    "\n",
    "Many methods exist to create this, CBOW and skipgram, and many implementations also exist. Here used is Gensim's word2vec, recent developments are subtantialy faster and deliver outstanding results.\n",
    "\n",
    "It can be used in word prediction and phrase generation by taking into account not just what words was before, but what kind of word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Will train a model with fasttext with all the scripts available, start by generating a single file with all the texts. In this case it will be using skipgram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import string\n",
    "\n",
    "df = pd.read_csv('data/dfreps0.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate the model on Gensim, now using per instance training instead of the whole text.\n",
    "\n",
    "Deliver sequentially the training texts, all the scripts in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/luiscastro/Library/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/gensim/utils.py:1015: UserWarning: Pattern library is not installed, lemmatization won't be available.\n",
      "  warnings.warn(\"Pattern library is not installed, lemmatization won't be available.\")\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec, Phrases\n",
    "from nltk.corpus   import stopwords\n",
    "import logging\n",
    "import os\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',level=logging.INFO)\n",
    "\n",
    "class trainer(object):\n",
    "    def __init__(self,dirname,index):\n",
    "        self.dirname = dirname\n",
    "        self.index   = index\n",
    "        self.cache   = stopwords.words('english')\n",
    " \n",
    "    def __iter__(self):\n",
    "        for fname in os.listdir(self.dirname):\n",
    "            if fname in self.index:\n",
    "                for line in open(os.path.join(self.dirname, fname)):\n",
    "                   #line  = line.translate(None,string.punctuation).split() #Remove punctuation\n",
    "                   #nline = []\n",
    "                   #for word in line:\n",
    "                   #    if word.lower() not in self.cache:                  #Remove stopwords\n",
    "                   #        nline.append(word)\n",
    "                   #yield nline\n",
    "                   yield line.translate(None,string.punctuation).split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentences = trainer('scripts/',df.index)\n",
    "model     = Word2Vec(sentences,size=200,min_count=5,iter=10,workers=2)\n",
    "#model.train(more_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save model and load it, and train it with more sentences if needed, we will just be querying now, to trim unneeded model memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.save('data/gensim_punct_5_100')\n",
    "\n",
    "#model = Word2Vec.load('data/gensim_punct_5_10')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment with the uses of the model. First check similarity between words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.64670523650250278"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.similarity('Batman','Superman')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, check which word doesn't match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Aquaman'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.doesnt_match(\"Batman Robin Superman Aquaman\".split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vector addition and subtraction. Which word is the closest to a sum and/or subtraction of words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating word vectors with movie scripts bears intriguing results. \n",
    "\n",
    "Batman - man = X\n",
    "\n",
    "The word man has different semantic interpretations can be the signs \n",
    "of age or seniority (Nightwing, Robin), of gender (Leia), moral compass \n",
    "(Monster) or of humanity altogether (Cyborg, Hal, Yoda). All resemble \n",
    "characteristics of the Dark Knight, but lack what we subtracted from 'Batman'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Robin', 0.4148382544517517),\n",
       " ('KnoxJohnston', 0.4054247736930847),\n",
       " ('Nightwing', 0.4025588035583496),\n",
       " ('Cyborg', 0.3848974108695984),\n",
       " ('Starfire', 0.3841334581375122),\n",
       " ('yoda', 0.3667193353176117),\n",
       " ('Superman', 0.33921581506729126),\n",
       " ('Spoony', 0.3350698947906494),\n",
       " ('Batmans', 0.3302323818206787),\n",
       " ('WHOOSHING', 0.32674193382263184)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive=['Batman'],negative=['man'],topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And just accesing the vector for the word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.5261097 ,  0.46229416,  0.7212823 ,  0.45801175, -0.46808067], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model['King'][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test accuracy of the model against Google test, question-words.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#acc = model.accuracy('trunk/questions-words.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create bigrams, sentences of 2 words like New York and San Francisco."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bigram_transformer = Phrases(sentences)\n",
    "model              = Word2Vec(bigram_transformer[sentences],size=100,min_count=10,workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If done with training, trim unused model memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to find the optimal way to generate the vectors within the scripts space. Easily could be to download training corpus to accoplish this, but the interest is on the words within the script corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next section: [Pending]()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
