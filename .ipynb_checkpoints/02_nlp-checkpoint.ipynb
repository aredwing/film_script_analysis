{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Film Script Analyzer\n",
    "\n",
    "## Data preparation\n",
    "\n",
    "## NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data will be generated by applying NLP tools to the scripts and extracting valuable statistics from them using the NLTK library.\n",
    "\n",
    "- **Words:** Total number of words in the script; a measure of the length of the script.\n",
    "- **Diversity:** Total number of unique words / total number of words; a measure of diversity of language.\n",
    "- **Length:** Mean word length on the script.\n",
    "- **Parts of speech:** Normalized counted parts of speech: **Verb, Noun, Adp, Adj, Conj, Pron, Prt, Num, Punc, X.**\n",
    "\n",
    "The function **'words'** is defined to generate this values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "speech = ['WORDS','DIVERSITY','LENGTH','VERB','NOUN','ADP','.','ADJ','ADV','CONJ','PRON','PRT','NUM','X']\n",
    "\n",
    "def words(script,speech):\n",
    "    tokens     = word_tokenize(script)\n",
    "    nwords     = len(tokens)\n",
    "    \n",
    "    if nwords != 0:\n",
    "        diversity  = len(set(tokens))/float(nwords)\n",
    "        tagger     = pos_tag(tokens,tagset='universal')\n",
    "        wordL      = 0.\n",
    "        \n",
    "        for i in tokens:\n",
    "            wordL += len(i)\n",
    "        wordL      = wordL/nwords\n",
    "\n",
    "        nspeech    = speech[3:]\n",
    "        counter    = [0.]*len(nspeech)\n",
    "        for i in tagger:\n",
    "            for j in range(len(nspeech)):\n",
    "                if i[1] == nspeech[j]:\n",
    "                    counter[j] += 1.\n",
    "                    \n",
    "        temp = [nwords,diversity,wordL]\n",
    "\n",
    "        for num in counter:\n",
    "            temp.append(num/nwords)\n",
    "\n",
    "        return temp\n",
    "\n",
    "    else:\n",
    "        return ['NaN']*len(speech)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a dataset to contain words information, the index is the index of the previous dataset and the columns are the new features created by the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_nlp = pd.DataFrame(index=df.index,columns=speech)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proceed to fill the information to the df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for name in df_nlp.index:\n",
    "    if pd.isnull(df_nlp['WORDS'].ix[name]):\n",
    "        text = open('scripts/'+name,'r')\n",
    "        df_nlp.ix[name] = words(text.read().encode(encoding='utf-8',errors='ignore'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatenate the new dataset with the previous one and save it, removing first all the instances where 'WORDS' is equal to NaN, that could happen because of a corrupted file or the script wasn't available when scrapped. There where also instances where a summary of the script was in the page instead of the whole script, errase those instances too. It will be important later too since Watson's API will not work with fewer than 100 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df2 = pd.concat([df_nlp,df],axis=1)\n",
    "df2 = df2.ix[df2['WORDS']!='NaN']\n",
    "df2 = df2.ix[df2['WORDS']>=1000]\n",
    "df2.to_csv('new/data/df2.csv',encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next section: [Watson](https://github.com/luisecastro/film_script_analysis/blob/master/03_watson.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
