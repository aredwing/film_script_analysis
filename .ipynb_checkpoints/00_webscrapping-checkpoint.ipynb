{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Film Script Analyzer\n",
    "\n",
    "## Data preparation\n",
    "\n",
    "### Webscrapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All scripts were scrapped from the following page: [SprinfieldSpringfield.](http://www.springfieldspringfield.co.uk/)\n",
    "\n",
    "Proceed to scrap the information from the pages, and parse it to *UTF-8* encoded text.\n",
    "\n",
    "The **'fetch'** function requests and receives the contents of a webpage throws and exception if finds a problem, it is used to scrape the web for information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import operator\n",
    "import requests\n",
    "import string\n",
    "import sys\n",
    "\n",
    "def fetch(address):\n",
    "    res = requests.get(address)\n",
    "    try:\n",
    "        res.raise_for_status()\n",
    "    except Exception as exc:\n",
    "        print('Problem: %s'%(exc))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the source URL, the variations are in the page number and the page letter, which is the starting letter of the script name. Collect the ranges or number of pages for all the initial letters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import bs4\n",
    "import json\n",
    "\n",
    "url       = [\"http://www.springfieldspringfield.co.uk/movie_scripts.php?order=\",\"&page=\"]\n",
    "letters   = string.ascii_uppercase\n",
    "charNum   = {}\n",
    "\n",
    "for i in list('0'+letters):\n",
    "    start = '1'\n",
    "    main = bs4.BeautifulSoup(fetch(url[0]+i+url[1]+start).text,'lxml')\n",
    "    temp = main.find_all('a')\n",
    "    for j in temp[-1:]:\n",
    "        num = str(j.contents[0]).encode('utf8')\n",
    "        charNum.update({i:int(num)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the Letter/Length pair is available, proceed to scrap the links. In order to not overload the server it, randomly select one number for each letter, that is, one page randomy visited for per letter. That should serve the purpose of a statistically valid sampling.\n",
    "\n",
    "The URLs are returned without the root that needs to be added. A dictionary is defined to hold the name of the movie and its link these links contain the scripts and then is saved to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy  as np\n",
    "\n",
    "prefix = 'http://www.springfieldspringfield.co.uk'\n",
    "pages  = {}\n",
    "\n",
    "for char in charNum.keys():\n",
    "    for num in range(1,charNum[char]+1):\n",
    "        temp = bs4.BeautifulSoup(fetch(url[0]+char+url[1]+str(num)).text,'lxml')\n",
    "        aLinks = temp.find_all('a',class_='script-list-item')\n",
    "        clean = ''\n",
    "        for link in aLinks:\n",
    "            pages.update({link.contents[0].encode('utf-8'):prefix+link.get('href')})\n",
    "            \n",
    "with open('data/pages.json','w') as f:\n",
    "    json.dump(pages,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform the web scrapping specifically from this page. The **'springScrap'** function is defined. This function is customized to extract the information from the Springfield page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def springScrap(raw):\n",
    "    result = ' '\n",
    "    soup = bs4.BeautifulSoup(raw.text,'lxml')\n",
    "    for e in soup.findAll('br'):\n",
    "        e.extract()\n",
    "    for text in soup.find_all('div',class_='scrolling-script-container'):\n",
    "        result += text.get_text().encode('utf8')\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iteratively, for each name in the pages dictionary, request the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for name, url in zip(pages.keys(),pages.values()):\n",
    "    temp = springScrap(fetch(url))\n",
    "    f = open('scripts/'+''.join(e for e in name if e.isalnum() or e == ' ')+'.txt','w')\n",
    "    f.write(temp)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A total of **17,271** scripts wer stored in disk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next section: [IMDB](https://github.com/luisecastro/film_script_analysis/blob/master/01_imdb.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
