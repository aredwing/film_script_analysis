{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Film Script Analyzer\n",
    "\n",
    "##Â Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summarization is a method to extract meaningful information from a bigger source. In the case of scripts, it is to extract sentences that can uniquely identify the script, sentences that capture the essence of the script.\n",
    "\n",
    "In order to do it, a statistical method is used. The frequencies of words are calculated, very high frequency words usually do not have a lot of information (like stopwords), very low frequency words may not convey the spirit of the script, they may very well be isolated instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize,word_tokenize\n",
    "from nltk.corpus   import stopwords\n",
    "from collections   import defaultdict\n",
    "from string        import punctuation\n",
    "from heapq         import nlargest\n",
    "\n",
    "def frequencies(word_sent,min_cut=0.1,max_cut=0.9):              #Set frequencies cut offs\n",
    "    stopw = set(stopwords.words('english')+list(punctuation))    #Create set of stopwords and punctuation to remove\n",
    "    freq  = defaultdict(int)                                     #Create dictionary for frequencies\n",
    "    \n",
    "    for s in word_sent:                                          #For each sentence in word sentences\n",
    "        for word in s:                                           #For each word in sentence\n",
    "            if word not in stopw:                                #If word is valid\n",
    "                freq[word] += 1                                  #Count it\n",
    "    m = float(max(freq.values()))                                #Get maximum value (for normalization)\n",
    "    \n",
    "    for w in freq.keys():                                        #For each word  \n",
    "        freq[w] = freq[w]/m                                      #Normalize\n",
    "        if freq[w] >= max_cut or freq[w] <= min_cut:             #Is word within the cutoff range?\n",
    "            del freq[w]                                          #Delete if not\n",
    "            \n",
    "    return freq                                                  #Return dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then to summarize, set thresholds of minimum and maximum frequencies and then choose from among the remaining sentences the top ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rank(ranking,n):                                             #Receives a list of sentences and frequencies\n",
    "    return nlargest(n,ranking,key=ranking.get)                   #Returns highest rated\n",
    "\n",
    "def summarize(text,n=5):                                         #Main function, receives text and #of sentences \n",
    "    sents      = sent_tokenize(text)                             #Splits text into sentences (token sentences) \n",
    "    assert n  <= len(sents)                                      #Number of total sentences to return at least n\n",
    "    word_sent  = [word_tokenize(s.lower()) for s in sents]       #Tokenize words from sentences\n",
    "    freq       = frequencies(word_sent)                          #Send bag of words to frequencies function\n",
    "    ranking    = defaultdict(int)                                #Create rank dictionary\n",
    "    \n",
    "    for i,sent in enumerate(word_sent):                          #For each tokenized sentence\n",
    "        for w in sent:                                           #For each word in sentence\n",
    "            if w in freq:                                        #if word is in frequency dictionary\n",
    "                ranking[i] += freq[w]                            #add its normalized frequency to the total\n",
    "                \n",
    "    sents_idx = rank(ranking,n)                                  #Send to rank, which will return n highest ranked sentences\n",
    "    \n",
    "    return [sents[j] for j in sents_idx]                         #Return all sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open text and try it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"I'm going to Italy and then l'm going to David's guru's ashram in lndia... ...and l'm going to end the year in Bali.\",\n",
       " \"It's long, it's tedious, I can't keep up... ...and l get these insane anxieties about everything in my life... ...and l've lost my place.\",\n",
       " 'And it was such a foreign concept to me, that l swear l almost began with: \"l\\'m a big fan of your work.\"',\n",
       " \"-No, I don't even have my-- l-- You don't have your-- You don't-- You're so naked.\",\n",
       " \"And l-- You know, I don't-- I don't know.\",\n",
       " \"Do not tell me what lessons l have and haven't learned in the last year... ...and don't tell me how balanced and wise you are... ...and how I can't express myself.\",\n",
       " \"If it wasn't for you, I wouldn't have come back to Bali... ...and l wouldn't have come back to myself.\",\n",
       " \"l'm sorry l didn't call sooner.\",\n",
       " \"I don't know why we can't accept... ...we don't wanna live in unhappiness anymore.\",\n",
       " \"You know, it's been a rough day, and if no one takes it personally... ...l'm going to take my large meal someplace else to eat it in silence.\"]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = open('scripts/Eat Pray Love 2010.txt','r').readlines()[2]\n",
    "\n",
    "summarize(text,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next Section: [word2vec](https://github.com/luisecastro/film_script_analysis/blob/master/11_word2vec.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
