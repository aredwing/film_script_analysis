{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Film Script Analyzer\n",
    "\n",
    "##Â Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summarization is a method to extract meaningful information from a bigger source. In the case of scripts, it is to extract sentences that can uniquely identify the script, sentences that capture the essence of the script.\n",
    "\n",
    "In order to do it, a statistical method is used. The frequencies of words are calculated, very high frequency words usually do not have a lot of information (like stopwords), very low frequency words may not convey the spirit of the script, they may very well be isolated instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize,word_tokenize\n",
    "from nltk.corpus   import stopwords\n",
    "from collections   import defaultdict\n",
    "from string        import punctuation\n",
    "from heapq         import nlargest\n",
    "\n",
    "def frequencies(word_sent,min_cut=0.1,max_cut=0.9):\n",
    "    stopw = set(stopwords.words('english')+list(punctuation))\n",
    "    freq  = defaultdict(int)\n",
    "    \n",
    "    for s in word_sent:\n",
    "        for word in s:\n",
    "            if word not in stopw:\n",
    "                freq[word] += 1\n",
    "    m = float(max(freq.values()))\n",
    "    \n",
    "    for w in freq.keys():\n",
    "        freq[w] = freq[w]/m\n",
    "        if freq[w] >= max_cut or freq[w] <= min_cut:\n",
    "            del freq[w]\n",
    "            \n",
    "    return freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then to summarize, set thresholds of minimum and maximum frequencies and then choose from among the remaining sentences the top ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rank(ranking,n):\n",
    "    return nlargest(n,ranking,key=ranking.get)\n",
    "\n",
    "def summarize(text,n=5):\n",
    "    sents      = sent_tokenize(text)\n",
    "    assert n  <= len(sents)\n",
    "    word_sent  = [word_tokenize(s.lower()) for s in sents]\n",
    "    freq       = frequencies(word_sent)\n",
    "    ranking    = defaultdict(int)\n",
    "    \n",
    "    for i,sent in enumerate(word_sent):\n",
    "        for w in sent:\n",
    "            if w in freq:\n",
    "                ranking[i] += freq[w]\n",
    "                \n",
    "    sents_idx = rank(ranking,n)  \n",
    "    \n",
    "    return [sents[j] for j in sents_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open text and try it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"I'm going to Italy and then l'm going to David's guru's ashram in lndia... ...and l'm going to end the year in Bali.\",\n",
       " \"It's long, it's tedious, I can't keep up... ...and l get these insane anxieties about everything in my life... ...and l've lost my place.\",\n",
       " 'And it was such a foreign concept to me, that l swear l almost began with: \"l\\'m a big fan of your work.\"',\n",
       " \"-No, I don't even have my-- l-- You don't have your-- You don't-- You're so naked.\",\n",
       " \"And l-- You know, I don't-- I don't know.\",\n",
       " \"Do not tell me what lessons l have and haven't learned in the last year... ...and don't tell me how balanced and wise you are... ...and how I can't express myself.\",\n",
       " \"If it wasn't for you, I wouldn't have come back to Bali... ...and l wouldn't have come back to myself.\",\n",
       " \"l'm sorry l didn't call sooner.\",\n",
       " \"I don't know why we can't accept... ...we don't wanna live in unhappiness anymore.\",\n",
       " \"You know, it's been a rough day, and if no one takes it personally... ...l'm going to take my large meal someplace else to eat it in silence.\"]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = open('scripts/Eat Pray Love 2010.txt','r').readlines()[2]\n",
    "\n",
    "summarize(text,10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
