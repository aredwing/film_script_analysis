{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommend\n",
    "\n",
    "This systems work primarly in 2 ways. \n",
    "- Analyze customers behavior and look at similarities among them. \n",
    "    - Ex. Cust1 likes (A,B). Cust2 likes (A,C). Cust1 may enjoy 'C'.\n",
    "- Look and things that have similar characteristics or that are associated. \n",
    "    - Ex. Cust1 likes A. A has (a,b,c). C has (a,b,d). Cust1 may enjoy 'C'. \n",
    "    \n",
    "Two interesting questions rise, how to know what customers like? what characteristics are important to create associations.\n",
    "\n",
    "- How to know what customers like?\n",
    "    - Primarily by votes (likes), ratings (4 stars) or browsed items.\n",
    "- What characteristics are important to create associations? \n",
    "    - That needs more substantive expertise. Smurfs are blue, I like smurfs but I may not enjoy blue scarfs. Fortunately since the comparation is just text we dwell in a single domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('data/recs.json') as data_file:    \n",
    "    recs = json.load(data_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since keys can be difficult to be found, including extra characters define **'levDist'** and **'levSearch'** to find them. They calculate the Levenshtein distance and return the closest (minimum edit distance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def levDist(s1,s2):\n",
    "    if len(s1) > len(s2):\n",
    "        s1, s2 = s2, s1\n",
    "\n",
    "    distances = range(len(s1) + 1)\n",
    "    for i2, c2 in enumerate(s2):\n",
    "        distances_ = [i2+1]\n",
    "        for i1, c1 in enumerate(s1):\n",
    "            if c1 == c2:\n",
    "                distances_.append(distances[i1])\n",
    "            else:\n",
    "                distances_.append(1 + min((distances[i1], distances[i1 + 1], distances_[-1])))\n",
    "        distances = distances_\n",
    "    return distances[-1]\n",
    "\n",
    "def levSearch(s1,dic):\n",
    "    result = []\n",
    "    count  = 99\n",
    "    for key in dic.keys():\n",
    "        keyn = \" \".join(key[:-9].lower().split())\n",
    "        s1n  = \" \".join(s1.lower().split())\n",
    "        #if len(keyn) == len(s1n):\n",
    "        temp = levDist(s1n,keyn)\n",
    "        if  temp < count:\n",
    "            count  = temp\n",
    "            result = key \n",
    "    return result\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'Last Chance Harvey 2008.txt'"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "levSearch('cast      dhance      varvey',recs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then define the **'recommender'** function. It takes the name of a script and recommends the closet or more similar scripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def recommender(string,dic,n=3):\n",
    "    s1 = levSearch(string,dic)\n",
    "    #print s1\n",
    "    return dic[s1][:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last Chance Harvey 2008.txt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[u'Jack and Jill 2011.txt', 0.9963078564004061],\n",
       " [u'Barefoot 2014.txt', 0.9948440050605697],\n",
       " [u'Ricki and the Flash 2015.txt', 0.9930041706830336]]"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recommender('SFbeat      dhance      varvey',recs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarize \n",
    "\n",
    "In order to know more about a script, it is always commendable to have a glimpse into it, to have basic information to understand it. Define the **'summarize'** class which summarizes and brings additional information about a script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize,word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from collections import defaultdict\n",
    "from string import punctuation\n",
    "from heapq import nlargest\n",
    "\n",
    "class FrequencySummarizer:\n",
    "    def __init__(self, min_cut=0.1, max_cut=0.9):\n",
    "        self._min_cut = min_cut\n",
    "        self._max_cut = max_cut \n",
    "        self._stopwords = set(stopwords.words('english') + list(punctuation))\n",
    "\n",
    "    def _compute_frequencies(self,word_sent):\n",
    "        freq = defaultdict(int)\n",
    "        for s in word_sent:\n",
    "            for word in s:\n",
    "                if word not in self._stopwords:\n",
    "                    freq[word] += 1\n",
    "        m = float(max(freq.values()))\n",
    "        for w in freq.keys():\n",
    "            freq[w] = freq[w]/m\n",
    "            if freq[w] >= self._max_cut or freq[w] <= self._min_cut:\n",
    "                del freq[w]\n",
    "        return freq\n",
    "\n",
    "    def summarize(self,s1,n=5):\n",
    "        s2 = levSearch(s1,recs)\n",
    "        text = open('scrapped/'+s2,'r').readlines()[2]\n",
    "        sents = sent_tokenize(text)\n",
    "        assert n <= len(sents)\n",
    "        word_sent = [word_tokenize(s.lower()) for s in sents]\n",
    "        self._freq = self._compute_frequencies(word_sent)\n",
    "        ranking = defaultdict(int)\n",
    "        for i,sent in enumerate(word_sent):\n",
    "            for w in sent:\n",
    "                if w in self._freq:\n",
    "                    ranking[i] += self._freq[w]\n",
    "        sents_idx = self._rank(ranking, n)  \n",
    "        print 'Title: ', df['Title'].ix[s2],\n",
    "        print '\\nRated: ', df['Rated'].ix[s2],\n",
    "        print '\\nRuntime: ', df['Runtime'].ix[s2],\n",
    "        print '\\nDirector: ', df['Director'].ix[s2],\n",
    "        print '\\nActors: ', df['Actors1'].ix[s2]+', '+df['Actors2'].ix[s2],\n",
    "        print '\\nGenres: ', df['Genre1'].ix[s2]+', '+df['Genre2'].ix[s2],\n",
    "        print '\\nRating: ', df['imdbRating'].ix[s2]\n",
    "        print '\\nPoster: ', df['Poster'].ix[s2]\n",
    "        print '\\nSummary:'\n",
    "        temp = [sents[j] for j in sents_idx]\n",
    "        for i in temp:\n",
    "            print i\n",
    "        print '\\n'\n",
    "        print 'Similar Titles:'\n",
    "        temp = recommender(s1,recs,5)\n",
    "        for i in temp:\n",
    "            print i[0][:-8]\n",
    "        print '\\n'\n",
    "\n",
    "    def _rank(self, ranking, n):\n",
    "        return nlargest(n, ranking, key=ranking.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title:  Eat Pray Love \n",
      "Rated:  PG-13 \n",
      "Runtime:  133.0 \n",
      "Director:  Ryan Murphy \n",
      "Actors:  Julia Roberts,  I. Gusti Ayu Puspawati \n",
      "Genres:  Drama,  Romance \n",
      "Rating:  5.7\n",
      "\n",
      "Poster:  https://images-na.ssl-images-amazon.com/images/M/MV5BMTY5NDkyNzkyM15BMl5BanBnXkFtZTcwNDQyNDk0Mw@@._V1_SX300.jpg\n",
      "\n",
      "Summary:\n",
      "I'm going to Italy and then l'm going to David's guru's ashram in lndia... ...and l'm going to end the year in Bali.\n",
      "It's long, it's tedious, I can't keep up... ...and l get these insane anxieties about everything in my life... ...and l've lost my place.\n",
      "And it was such a foreign concept to me, that l swear l almost began with: \"l'm a big fan of your work.\"\n",
      "-No, I don't even have my-- l-- You don't have your-- You don't-- You're so naked.\n",
      "And l-- You know, I don't-- I don't know.\n",
      "\n",
      "\n",
      "Similar Titles:\n",
      "Me Again \n",
      "In The French Style \n",
      "Under the Greenwood Tree \n",
      "Far from the Madding Crowd \n",
      "Dames \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fs = FrequencySummarizer()\n",
    "fs.summarize('beat     spay    blove',5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorize\n",
    "\n",
    "Use machine learning to assing a genre to the movies. Use the scaled dataset to test different models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble  import RandomForestClassifier as RFC\n",
    "from sklearn.neighbors import KNeighborsClassifier as KNC\n",
    "from sklearn.ensemble  import AdaBoostClassifier as ABC\n",
    "from sklearn.svm       import SVC\n",
    "\n",
    "def randFor(train,target,test,n,d,f):\n",
    "    clf = RFC(n_estimators=n,max_depth=d,max_features=f)\n",
    "    clf.fit(train,target)\n",
    "    return clf.predict(test)\n",
    "\n",
    "def kNearN(train,target,test,k,w):\n",
    "    clf = KNC(n_neighbors=k,weights=w)\n",
    "    clf.fit(train,target)\n",
    "    return clf.predict(test)\n",
    "\n",
    "def suppVC(train,target,test,k,g,c):\n",
    "    clf = SVC(kernel=k,gamma=g,C=c)\n",
    "    clf.fit(train,target)\n",
    "    return clf.predict(test)\n",
    "\n",
    "def aBoost(train,target,test,b,n,l,a):\n",
    "    clf = ABC(base_estimator=b,n_estimators=n,learning_rate=l,algorithm=a)\n",
    "    clf.fit(train,target)\n",
    "    return clf.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score as cvs\n",
    "\n",
    "clfKN = KNR(n_neighbors=13,weights='uniform')\n",
    "clfSV = SVR(kernel='rbf',C=5.0,gamma=0.21)\n",
    "clfRF = RandomForestRegressor(n_estimators=200,max_depth=5,max_features='auto')\n",
    "\n",
    "scoresKN = cross_val_score(clfKN,dfKK,targets,cv=5,scoring=mse_scorer)\n",
    "scoresSV = cross_val_score(clfSV,dfNum,targets,cv=5,scoring=mse_scorer)\n",
    "scoresRF = cross_val_score(clfRF,dfNum,targets,cv=5,scoring=mse_scorer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
