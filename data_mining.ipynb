{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Predict IMDB rating by extracting information from script\n",
    "# This can work 2 ways, one a script could be automatically\n",
    "# analyzed, this could show the opportunities of success\n",
    "# for the script, producers could use it to set a standard\n",
    "# for the scripts to read. By using metadata other parameters\n",
    "# could be calculated and optimized when creating the movie\n",
    "# like duration, actors, rating among others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Functions that will be used\n",
    "import helper as hp\n",
    "import pandas as pd\n",
    "import numpy  as np\n",
    "import string\n",
    "from sklearn.linear_model import LinearRegression as lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Select randomly starting letter of movie (later will be all letters)\n",
    "np.random.seed(0)\n",
    "n = 1\n",
    "letters   = list(np.random.choice(list(string.ascii_uppercase),size=n,replace=False))\n",
    "url       = [\"http://www.springfieldspringfield.co.uk/movie_scripts.php?order=\",\"&page=\"]\n",
    "\n",
    "# Append data obtained to empty list\n",
    "# fetch the url, and extract the links from it\n",
    "# these links contain the droids we are looking for... i mean the scripts\n",
    "prefix = 'http://www.springfieldspringfield.co.uk'\n",
    "pages  = {}\n",
    "n      = 3\n",
    "for i in letters:\n",
    "    for j in range(1,n): # Number of pages to go in, change latter too\n",
    "        temp = bs4.BeautifulSoup(hp.fetch(url[0]+i+url[1]+str(j)).text,'lxml')\n",
    "        temp = temp.find_all('a',class_='script-list-item')\n",
    "        clean = ''\n",
    "        for link in temp:\n",
    "            pages.update({str(link.contents[0]):prefix+link.get('href')})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Having the list of url's, got to each one and scrape the script\n",
    "script = []\n",
    "for i in pages.values():\n",
    "    script.append(hp.springScrap(hp.fetch(i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Process script to remove all caps words, indicating actions\n",
    "# to be seen if leaving or removing them creates a better model\n",
    "scriptRC = []\n",
    "for i in range(len(script)):\n",
    "    scriptRC.append(hp.removeCAPS(script[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save text of scrapped and process scripts to local disk\n",
    "for i in range(len(script)):\n",
    "    f = open('scrapped/'+pages.keys()[i]+'.txt','w')\n",
    "    f.write(script[i])\n",
    "    f.close()\n",
    "    \n",
    "for i in range(len(script)):\n",
    "    f = open('scrapped/'+pages.keys()[i]+'RC.txt','w')\n",
    "    f.write(scriptRC[i])\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Username and password for Watson's API, obtained\n",
    "# Creating an account with them, some free perks\n",
    "iusername = 'ABCDEF'\n",
    "ipassword = '123456'\n",
    "\n",
    "# Submit scripts (in this case those with actions removed)\n",
    "# to Watson through the API, the API is for Personality Insights\n",
    "# it returns a set of 30 parameters obtained by analysing\n",
    "# the text supplied\n",
    "insights = []\n",
    "for i in scriptRC:\n",
    "    insights.append(hp.insight(i,iusername,ipassword))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Associate names of movie scripts with insights returned\n",
    "nins = []\n",
    "for i in range(len(insights)):\n",
    "    nins.append([pages.keys()[i],hp.dToL(hp.flatten(insights[i]))[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a dataframe, the columns will be the each of the insights features\n",
    "# the rows each instant of movie script, it starts with 0's\n",
    "index   = pages.keys()\n",
    "columns = hp.dToL(hp.flatten(insights[0]))[1]\n",
    "\n",
    "df = pd.DataFrame(data=np.zeros(shape=(len(index),len(columns))),index=index,columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Fill the dataframe with the information from the insights\n",
    "for i in range(len(index)):\n",
    "    for j in range(len(columns)):\n",
    "        df.ix[(i,j)] = nins[i][1][j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# After completing creating data from Watson, proceed to use OMDB API, this API\n",
    "# returns important data from a movie by supplying the name of the movie or\n",
    "# the imdb tag.\n",
    "# The names of the movies were available from the previous scrapping, however\n",
    "# names sometimes are spelled or written differently and could not be found by the\n",
    "# API, that is why this was checked by hand.\n",
    "#\n",
    "# Some curation of the titles needs to be done by hand, will automate (somehow) later\n",
    "titles = ['Ca$h','Caddyshack','Cabin Fever 3: Patient Zero','Caged','Cairo Time','Caddyshack II','Cable Guy',\n",
    "         'Caffeine','Caligula','Calendar Girls','Cabin in the sky','The White Horse','Cadaver','Cabin Fever',\n",
    "         'Call Girl','C.r.a.z.y','Julius Caesar','Caesar and Cleopatra','Cake','Cabin Fever 2','California Solo',\n",
    "         'Cabaret Desire','C.O.G.','Cafe Society','Cadillac Records','Cabin in the woods','Cabaret',\n",
    "         'Illustrious Corpses','Cadillac Man','Cahill','Cake Eaters','Calamity Jane','The Dark Knight','The Godfather',\n",
    "         'Fight Club','The Lord of the Rings: The Fellowship of the Ring',\n",
    "          'Star Wars: Episode V - The Empire Strikes Back','The Matrix','The silence of the lambs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set the dataframe index to the curated names\n",
    "df.index = titles\n",
    "\n",
    "# Proceed to request the information to the OMDB API\n",
    "# and store it in a dictionary\n",
    "imdbDict= {}\n",
    "for i in titles:\n",
    "    imdbDict.update({i:omdb(name=i)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Our target is in sight, the 'imdbRating' is what we aim to \n",
    "# predict through regression. Create the target and store\n",
    "# it in memory\n",
    "target = pd.DataFrame(index=df.index,columns=['imdbRating'])\n",
    "for i in df.index:\n",
    "    target.ix[i] = imdbDict[i]['imdbRating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Check if the rating was available, if not remove that row\n",
    "nas    = target['imdbRating']!='N/A'\n",
    "target = target.ix[nas]\n",
    "df = df.ix[nas]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# By using the nltk library for natural language processing\n",
    "# it is possible to obtain interesting statistics of the text\n",
    "# that can give further power to the model\n",
    "# analytics this time are # of words, diversity of words\n",
    "# which is unique words / total and #verbs / #nouns\n",
    "nanalytics     = []\n",
    "for i in range(len(scriptRC)):\n",
    "    nanalytics.append([pages.keys()[i],words(scriptRC[i])])\n",
    "\n",
    "# Store this new analytics in the training df\n",
    "df['Words']      = 0.\n",
    "df['DiversityW'] = 0.\n",
    "df['Verb/Noun']  = 0.\n",
    "\n",
    "for i in range(len(nanalytics)):\n",
    "    df['Words'].iloc[i]      = nanalytics[i][1][0]\n",
    "    df['DiversityW'].iloc[i] = nanalytics[i][1][1]\n",
    "    df['Verb/Noun'].iloc[i]  = nanalytics[i][1][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# From the imdb API information we use only 2 now,\n",
    "# The year and the runtime of the movie\n",
    "df['Year']    = 0\n",
    "df['Runtime'] = 0\n",
    "\n",
    "for i in df.index:\n",
    "    df['Year'][i]    = imdbDict[i]['Year']\n",
    "    df['Runtime'][i] = imdbDict[i]['Runtime'].split()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Finish by storing the dataset and the target on disk\n",
    "# for further use.\n",
    "df.to_csv('scrapped/final.csv')\n",
    "target.to_csv('scrapped/target.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The road to victory... is filled by rehearsals and mistakes.\n",
    "#\n",
    "#url1 = 'http://www.dailyscript.com/movie.html'\n",
    "#url2 = 'http://www.dailyscript.com/movie_n-z.html'\n",
    "#prefix = 'http://www.dailyscript.com/'\n",
    "\n",
    "#list1 = fetch(url1)\n",
    "#list2 = fetch(url2)\n",
    "\n",
    "#soup1 = bs4.BeautifulSoup(list1.text,'lxml')\n",
    "#soup2 = bs4.BeautifulSoup(list2.text,'lxml')\n",
    "#prefix = 'http://www.dailyscript.com/'\n",
    "\n",
    "#l1 = []\n",
    "#l2 = []\n",
    "#l4 = []\n",
    "\n",
    "#for i in soup1.find_all('a')[8:-3]:\n",
    "#    if i.contents[0].encode('utf8') != 'imdb':\n",
    "#        l1.append(i.contents[0].encode('utf8'))\n",
    "#        l2.append(prefix+i.get('href'))\n",
    "#    else:\n",
    "#        l3.append(i.contents[0].encode('utf8'))\n",
    "#        l4.append(i.get('href'))\n",
    "\n",
    "#l4.append('0') #There is somewhere something not cool, check it\n",
    "        \n",
    "#for i in soup2.find_all('a')[8:-3]:\n",
    "#    if i.contents[0].encode('utf8') != 'imdb':\n",
    "#        l1.append(i.contents[0].encode('utf8'))\n",
    "#        l2.append(prefix+i.get('href'))\n",
    "#    else:\n",
    "#        l4.append(i.get('href'))\n",
    "\n",
    "#temp = l1[0]\n",
    "#nList = []\n",
    "#pdfs  = []\n",
    "#nimdb = []\n",
    "\n",
    "# filter pdfs\n",
    "#for i in range(len(l2)):\n",
    "#    if l2[i][-3:].lower() != 'pdf':\n",
    "#        pdfs.append(i)\n",
    "\n",
    "# filter imdbs\n",
    "#for i in range(len(l4)):\n",
    "#    if l4[i] != '0':\n",
    "#        nimdb.append(i)\n",
    "\n",
    "#for i in range(2,11):\n",
    "#    raw  = fetch('http://www.springfieldspringfield.co.uk/movie_scripts.php?order=A&page='+str(i))\n",
    "#    soup = bs4.BeautifulSoup(temp.text,'lxml')\n",
    "#    for i in soup.findall('a')\n",
    "\n",
    "#soup1 = bs4.BeautifulSoup(list1.text,'lxml')\n",
    "#soup2 = bs4.BeautifulSoup(list2.text,'lxml')\n",
    "\n",
    "#for i in soup1.find_all('a')[8:-3]:\n",
    "#    if i.contents[0].encode('utf8') != 'imdb':\n",
    "#        l1.append(i.contents[0].encode('utf8'))\n",
    "#        l2.append(prefix+i.get('href'))\n",
    "#    else:\n",
    "#        l3.append(i.contents[0].encode('utf8'))\n",
    "#        l4.append(i.get('href'))\n",
    "\n",
    "#l4.append('0') #There is somewhere something not cool, check it\n",
    "        \n",
    "#for i in soup2.find_all('a')[8:-3]:\n",
    "#    if i.contents[0].encode('utf8') != 'imdb':\n",
    "#        l1.append(i.contents[0].encode('utf8'))\n",
    "#        l2.append(prefix+i.get('href'))\n",
    "#    else:\n",
    "#        l4.append(i.get('href'))\n",
    "\n",
    "#url       = ['http://www.springfieldspringfield.co.uk/movie_script.php?movie=dark-knight-the-batman-the-dark-knight',\n",
    "#             'http://www.springfieldspringfield.co.uk/movie_script.php?movie=godfather-the',\n",
    "#             'http://www.springfieldspringfield.co.uk/movie_script.php?movie=fight-club',\n",
    "#             'http://www.springfieldspringfield.co.uk/movie_script.php?movie=lord-of-the-rings-the-fellowship-of-the-ring-the',\n",
    "#             'http://www.springfieldspringfield.co.uk/movie_script.php?movie=star-wars-episode-v-the-empire-strikes-back',\n",
    "#             'http://www.springfieldspringfield.co.uk/movie_script.php?movie=matrix-the',\n",
    "#             'http://www.springfieldspringfield.co.uk/movie_script.php?movie=silence-of-the-lambs-the']\n",
    "\n",
    "#names     = ['darkknight','godfather','fightclub','lotrI','starwarsV','matrix','silencelambs']\n",
    "#raw       = fetch(url)\n",
    "#text      = springScrap(raw)\n",
    "#analysis  = insight(text,iusername,ipassword) \n",
    "#flat      = flatten(analysis)\n",
    "#saveText(text,name)\n",
    "\n",
    "# Courier 12\n",
    "\n",
    "#qual  = []\n",
    "\n",
    "#for i in range(len(names)):\n",
    "#    qual.append([i[1] for i in pd.read_csv('scrapped/'+names[i]+'insight.csv').values])\n",
    "\n",
    "#matrix = pd.DataFrame(qual,columns=qualities,index=names)\n",
    "#matrix['score'] = []\n",
    "#meta data \n",
    "#matrix\n",
    "\n",
    "#import pandas as pd\n",
    "#import numpy as np\n",
    "\n",
    "#qualities = ['Dutifulness','Cooperation','Self-consciousness','Orderliness','Achievement striving','Self-efficacy',\n",
    "#             'Activity level','Self-discipline','Excitement-seeking','Cautiousness','Morality','Anxiety','Emotionality',\n",
    "#             'Vulnerability','Immoderation','Sympathy','Friendliness','Modesty','Altruism','Assertiveness',\n",
    "#             'Adventurousness','Gregariousness','Intellect','Imagination','Artistic interests','Depression','Anger',\n",
    "#             'Trust','Cheerfulness','Liberalism']\n",
    "\n",
    "#url       = ['http://www.springfieldspringfield.co.uk/movie_script.php?movie=dark-knight-the-batman-the-dark-knight',\n",
    "#             'http://www.springfieldspringfield.co.uk/movie_script.php?movie=godfather-the',\n",
    "#             'http://www.springfieldspringfield.co.uk/movie_script.php?movie=fight-club',\n",
    "#             'http://www.springfieldspringfield.co.uk/movie_script.php?movie=lord-of-the-rings-the-fellowship-of-the-ring-the',\n",
    "#             'http://www.springfieldspringfield.co.uk/movie_script.php?movie=star-wars-episode-v-the-empire-strikes-back',\n",
    "#             'http://www.springfieldspringfield.co.uk/movie_script.php?movie=matrix-the',\n",
    "#             'http://www.springfieldspringfield.co.uk/movie_script.php?movie=silence-of-the-lambs-the']\n",
    "\n",
    "#names     = ['darkknight','godfather','fightclub','lotrI','starwarsV','matrix','silencelambs']\n",
    "\n",
    "#df = pd.DataFrame(np.zeros(shape=(len(qualities),1)),index=qualities)\n",
    "#df = df.transpose()\n",
    "\n",
    "#for i in range(len(url)):\n",
    "#    raw          = fetch(url[i])\n",
    "#    text         = springScrap(raw)\n",
    "#    analysis     = insight(text,iusername,ipassword) \n",
    "#    flat         = flatten(analysis)\n",
    "#    df[names[i]] = flat.values()\n",
    "#    saveText(text,names[i])\n",
    "#    pd.DataFrame(flat.values()).transpose().to_csv(names[i]+'insight.csv')\n",
    "#    saveText(flat,names[i]+'insight')\n",
    "\n",
    "# Data collected yesterday for tests, add to the df\n",
    "# names     = ['darkknight','godfather','fightclub','lotrI','starwarsV','matrix','silencelambs']\n",
    "\n",
    "# nlist2 = []\n",
    "# for i in names:\n",
    "#    nlist2.append(pd.read_csv('scrapped/'+i+'insight.csv'))\n",
    "\n",
    "#for i in range(len(names)):\n",
    "#    x = pd.DataFrame(data=np.transpose(list(nlist2[i]['0']))).transpose()\n",
    "#    x.columns = columns\n",
    "#    x.index   = [names[i]]\n",
    "#    df = df.append(x)\n",
    "# There is lots of information to be addressed, some information is may help the model, \n",
    "# test.keys()\n",
    "# Our target will be the imdbrating or the metascore, we want those\n",
    "# Futher information that might be useful would be rated, language, runtime, votes could be\n",
    "# used to filter rows with few votes, year, actors and directors could be used for simulations\n",
    "# what impact an actor or a director could have in the ratings\n",
    "# by now, lets take all the information and add it cautiosly to the model\n",
    "# df = df.drop(df.index[22])\n",
    "# df.index\n",
    "# To account for the rows i should have not deleted until now and add some missing:\n",
    "# nanalytics = []\n",
    "# for i in [0,1,2,3,4,5,6,7,8,9,10,12,14,15,16,17,18,19,20,21,22,24,25,26,27,28,29,30,31,32,33]:\n",
    "#     nanalytics.append(analytics[i])\n",
    "\n",
    "# tdk = words(encoder(hp.readTxt('scrapped/darkknight')))\n",
    "# tgf = words(encoder(hp.readTxt('scrapped/godfather')))\n",
    "# fca = words(encoder(hp.readTxt('scrapped/fightclub')))\n",
    "# lor = words(encoder(hp.readTxt('scrapped/lotrI')))\n",
    "# swv = words(encoder(hp.readTxt('scrapped/starwarsV')))\n",
    "# tma = words(encoder(hp.readTxt('scrapped/matrix')))\n",
    "# sol = words(encoder(hp.readTxt('scrapped/silencelambs')))\n",
    "\n",
    "# nanalytics.append(['darkknight',tdk])\n",
    "# nanalytics.append(['godfather',tgf])\n",
    "# nanalytics.append(['fightclub',fca])\n",
    "# nanalytics.append(['lordoftherings',lor])\n",
    "# nanalytics.append(['starwarsv',swv])\n",
    "# nanalytics.append(['matrix',tma])\n",
    "# nanalytics.append(['silenceoflambs',sol])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
