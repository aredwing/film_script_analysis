{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Film Script Analyzer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Data Collection/Preparation\n",
    "\n",
    "- **Webscrapping**\n",
    "- **Natural Language Processing**\n",
    "- **IMDB API**\n",
    "- **IBM Watson API**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Webscrapping\n",
    "\n",
    "All scripts were scrapped from the following page.\n",
    "\n",
    "- [SprinfieldSpringfield](http://www.springfieldspringfield.co.uk/)\n",
    "\n",
    "Proceed to scrap the information from the pages, and parse it to *UTF-8* encoded text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **'fetch'** function requests and receives the contents of a webpage throws and exception if finds a problem, it is used to scrape the web for information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import operator\n",
    "import requests\n",
    "import string\n",
    "import sys\n",
    "\n",
    "def fetch(address):\n",
    "    res = requests.get(address)\n",
    "    try:\n",
    "        res.raise_for_status()\n",
    "    except Exception as exc:\n",
    "        print('Problem: %s'%(exc))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the source URL, the variations are in the page number and the page letter, which is the starting letter of the script name. Collect the ranges for all the initial letters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import bs4\n",
    "import json\n",
    "\n",
    "url       = [\"http://www.springfieldspringfield.co.uk/movie_scripts.php?order=\",\"&page=\"]\n",
    "letters   = string.ascii_uppercase\n",
    "charNum   = {}\n",
    "\n",
    "for i in list('0'+letters):\n",
    "    start = '1'\n",
    "    main = bs4.BeautifulSoup(fetch(url[0]+i+url[1]+start).text,'lxml')\n",
    "    temp = main.find_all('a')\n",
    "    for j in temp[-1:]:\n",
    "        num = str(j.contents[0]).encode('utf8')\n",
    "        charNum.update({i:int(num)})\n",
    "\n",
    "\n",
    "with open('data/charNum.json','w') as f:\n",
    "    json.dump(charNum,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the Letter/Length pair is available, proceed to scrap the links. In order to not overload the server it, randomly select one number for each letter, that is, one page randomy visited for per letter. That should serve the purpose of a statistically valid sampling.\n",
    "\n",
    "The URLs are returned without the root that needs to be added. A dictionary is defined to hold the name of the movie and its link these links contain the scripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy  as np\n",
    "\n",
    "prefix = 'http://www.springfieldspringfield.co.uk'\n",
    "pages  = {}\n",
    "\n",
    "for i in [0,1,2,3]:\n",
    "    np.random.seed(i)\n",
    "    for k in charNum.keys():\n",
    "        j = np.random.choice(range(1,charNum[k]))\n",
    "        temp = bs4.BeautifulSoup(fetch(url[0]+k+url[1]+str(j)).text,'lxml')\n",
    "        aLinks = temp.find_all('a',class_='script-list-item')\n",
    "        clean = ''\n",
    "        for link in aLinks:\n",
    "            pages.update({str(link.contents[0]):prefix+link.get('href')})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform the web scrapping specifically from this page. The **'springScrap'** function is defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def springScrap(raw):\n",
    "    result = ' '\n",
    "    soup = bs4.BeautifulSoup(raw.text,'lxml')\n",
    "    for e in soup.findAll('br'):\n",
    "        e.extract()\n",
    "    for text in soup.find_all('div',class_='scrolling-script-container'):\n",
    "        result += text.get_text().encode('utf8')\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterarively, for each name in the pages dictionary, request the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "script = []\n",
    "for i in pages.values():\n",
    "    script.append(springScrap(fetch(i)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proceed to store everything on disk, remove punctuation and non alphanumeric characters from the names before saving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(len(script)):\n",
    "    f = open('scrapped/'+''.join(e for e in pages.keys()[i] if e.isalnum() or e == ' ')+'.txt','w')\n",
    "    f.write(script[i])\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A total of **2319** scripts are stored in the HD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the previously saved files to memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile\n",
    "\n",
    "script = {}\n",
    "onlyfiles = [f for f in listdir('scrapped/') if isfile('scrapped/'+f)]\n",
    "for i in onlyfiles:\n",
    "    with open('scrapped/'+i) as k:\n",
    "        script.update({i:[k.readlines(),[]]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data will be generated by applying NLP tools to the scripts and extracting valuable statistics from them using the NLTK library.\n",
    "\n",
    "- **Words:** Total number of words in the script; a measure of the length of the script.\n",
    "- **Diversity:** Total number of unique words / total number of words; a measure of diversity of language.\n",
    "- **Length:** Mean word length on the script.\n",
    "- **Parts of speech:** Normalized counted parts of speech: **Verb, Noun, Adp, Adj, Conj, Pron, Prt, Num, Punc, X.**\n",
    "\n",
    "The function **'words'** is defined to generate this values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "def words(script):\n",
    "    tokens    = nltk.word_tokenize(script)\n",
    "    nwords    = len(tokens)\n",
    "    diversity = len(set(tokens))/float(nwords)\n",
    "    tagger    = nltk.pos_tag(tokens,tagset='universal')\n",
    "    \n",
    "    wordL     = 0.\n",
    "    for i in tokens:\n",
    "        wordL += len(i)\n",
    "    wordL     = wordL/nwords\n",
    "    \n",
    "    speech    = ['VERB','NOUN','ADP','.','ADJ','ADV','CONJ','PRON','PRT','NUM','X']\n",
    "    counter   = [0.]*len(speech)\n",
    "    for i in tagger:\n",
    "        for j in range(len(speech)):\n",
    "            if i[1] == speech[j]:\n",
    "                counter[j] += 1.\n",
    "    \n",
    "    return [nwords,diversity,wordL,\n",
    "            counter[0]/nwords,counter[1]/nwords,counter[2]/nwords,counter[3]/nwords,\n",
    "            counter[4]/nwords,counter[5]/nwords,counter[6]/nwords,counter[7]/nwords,\n",
    "            counter[8]/nwords,counter[9]/nwords,counter[10]/nwords]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create dataframe to contain the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "speech = ['WORDS','DIVERSITY','LENGTH','VERB','NOUN','ADP','.','ADJ','ADV','CONJ','PRON','PRT','NUM','X']\n",
    "\n",
    "df = pd.DataFrame(index=script.keys(),columns=speech)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for film in df.index:\n",
    "    if len(script[film][0])<2:\n",
    "        pass\n",
    "    else:\n",
    "        temp = words(script[film][0][2])\n",
    "        for i in range(len(temp)):\n",
    "            df[speech[i]].ix[film] = temp[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMDB API\n",
    "\n",
    "[OMDB](https://www.omdbapi.com/) is an IMDB API interfase, it is used by submitting a query with film name or IMDB tag and answers with JSON data such as Year, Rating, Actors, Directors, IMDB rating, etc.\n",
    "\n",
    "Define a **'omdb'** function to submit the queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def omdb(tag=None,name=None):\n",
    "    if tag:\n",
    "        url = 'http://www.omdbapi.com/?i='+tag+'&plot=short&r=json'\n",
    "        raw = fetch(url)\n",
    "    else:\n",
    "        url = \"http://www.omdbapi.com/?t=\"+name+\"&y=&plot=short&r=json\"\n",
    "        raw = fetch(url)\n",
    "    result = ''\n",
    "    for i in raw:\n",
    "        result += i \n",
    "    return json.loads(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the index of the df, the names of the films, to request the data, but first remove the ' (year)'+'.txt' end part of the strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "imdb = {}\n",
    "for key in df.index:\n",
    "    name = ''.join(e for e in key if e.isalnum() or e == ' ')[:-8]\n",
    "    imdb.update({key:omdb(name=name)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proceed to save the file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('data/imbd.json','w') as f:\n",
    "    json.dump(imdb,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "columns = imdb['Anegan 2015.txt'].keys()\n",
    "index   = df.index\n",
    "tempDf  = pd.DataFrame(columns=columns,index=index)\n",
    "\n",
    "for row in index:\n",
    "    for col in columns:\n",
    "        if imdb[row]['Response']=='True':\n",
    "            tempDf[col][row] = imdb[row][col]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatenate it with the previous one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df2 = pd.concat([df,tempDf],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove non movies, scripts with less than a 1000 words, scripts with high rate of numbers (time indexed). Also remove all non movies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df2 = df2.ix[df2['WORDS']>1000]\n",
    "df2 = df2.ix[df2['NUM']<0.1]\n",
    "df2 = df2.ix[df2['Type']=='movie']\n",
    "df2 = df2.drop('Type',1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform features to easier to handle values:\n",
    "- **Language:** Leave only first language in list.\n",
    "- **Genre:** Include first two genres listed in different columns.\n",
    "- **Actors:** Include first two actors listed in different columns.\n",
    "- **Year:** Set it as an integer.\n",
    "- **Runtime:** Split minutes integer from 'min' string.\n",
    "\n",
    "Define **'split'** function for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split(x,y,n):\n",
    "    temp = str(x).split(y)\n",
    "    if len(temp)>1:\n",
    "        return str(x).split(y)[n:n+1][0]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df2['Runtime']  = df2['Runtime'].apply(split,args=(' ',0))\n",
    "df2['Actors1']  = df2['Actors'].apply(split,args=(',',0))\n",
    "df2['Actors2']  = df2['Actors'].apply(split,args=(',',1))\n",
    "df2['Year']     = df2['Year'].apply(int)\n",
    "df2['Genre1']   = df2['Genre'].apply(split,args=(',',0))\n",
    "df2['Genre2']   = df2['Genre'].apply(split,args=(',',1))\n",
    "df2['Language'] = df2['Language'].apply(split,args=(',',0))\n",
    "df2             = df2.drop('Actors',1)\n",
    "df2             = df2.drop('Genre',1)\n",
    "df2             = df2.ix[df2['Language']=='English']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IBM Watson's API\n",
    "\n",
    "\"IBM Watson is a technology platform that uses natural language processing and machine learning to reveal insights from large amounts of unstructured data\"\n",
    "\n",
    "Watson's interfase needs a [registered account](https://www.ibm.com/account/us-en/signup/register.html?a=MTBmNDg2NDktNDI2MC00&ctx=C001&trial=yes&catalogName=Master&quantity=1&partNumber=WA2PROTRIAL&source=mrsaas-trial-ibmid&pkg=ov49121&S_TACT=000000WB&S_OFF_CD=10000752&siteID=WA&watsonanalytics=true), which assigns a needed username / password.\n",
    "\n",
    "The process is as follows:\n",
    "- Authenticate with username/password\n",
    "- Submit text to be analized\n",
    "- Receive analysis of text, 30 features that describe the data.\n",
    "\n",
    "Such features are:\n",
    "![features](pi_viz.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assign variables with username / password."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from watson_developer_cloud import PersonalityInsightsV2 as PerIns\n",
    "\n",
    "iusername = 'ABCDE'\n",
    "ipassword = '12345'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define function **'insight'** to submit the queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def insight(text,user,password):\n",
    "    connect = PerIns(username=user,password=password)\n",
    "    return connect.profile(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Send text to analyze, store it in a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "insights = {}\n",
    "\n",
    "for row in df2.index:\n",
    "    temp    = open('scrapped/'+row,'r').read()\n",
    "    insights.update({row:insight(temp,iusername,ipassword)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define **'flatten'** function to extract data from the insights dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def flatten(orig):\n",
    "    data = {}\n",
    "    for c in orig['tree']['children']:\n",
    "        if 'children' in c:\n",
    "            for c2 in c['children']:\n",
    "                if 'children' in c2:\n",
    "                    for c3 in c2['children']:\n",
    "                        if 'children' in c3:\n",
    "                            for c4 in c3['children']:\n",
    "                                if (c4['category'] == 'personality'):\n",
    "                                    data[c4['id']] = c4['percentage']\n",
    "                                    if 'children' not in c3:\n",
    "                                        if (c3['category'] == 'personality'):\n",
    "                                                data[c3['id']] = c3['percentage']\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "finsights = {}\n",
    "\n",
    "for key in insights.keys():\n",
    "    finsights.update({key:flatten(insights[key])})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert it to dataframe and concatenate with the previous dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "columns = finsights.items()[0][1].keys()\n",
    "index   = df2.index\n",
    "tempDf  = pd.DataFrame(index=index,columns=columns)\n",
    "\n",
    "for row in tempDf.index:\n",
    "    for feature in finsights[row].keys():\n",
    "        tempDf[feature].ix[row] = finsights[row][feature]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df3 = pd.concat([tempDf,df2],axis=1)\n",
    "df3.to_csv('data/dataset.csv',encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the initial **2319** scrapped scripts, after filtering and cleaning, **1364** scripts were left with **65** features.\n",
    "\n",
    "Continue to [Data Visualization.](https://github.com/luisecastro/dataInc/blob/master/data_viz.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
